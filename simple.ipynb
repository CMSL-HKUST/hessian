{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde353af-a611-4b08-b3e8-22c23b5098a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       __       ___      ___   ___                _______  _______ .___  ___. \n",
      "      |  |     /   \\     \\  \\ /  /               |   ____||   ____||   \\/   | \n",
      "      |  |    /  ^  \\     \\  V  /      ______    |  |__   |  |__   |  \\  /  | \n",
      ".--.  |  |   /  /_\\  \\     >   <      |______|   |   __|  |   __|  |  |\\/|  | \n",
      "|  `--'  |  /  _____  \\   /  .  \\                |  |     |  |____ |  |  |  | \n",
      " \\______/  /__/     \\__\\ /__/ \\__\\               |__|     |_______||__|  |__| \n",
      "                                                                              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import jax.flatten_util\n",
    "import scipy\n",
    "import logging\n",
    "\n",
    "from jax_fem import logger\n",
    "from hessian.hessp import incremental_forward_and_adjoint\n",
    "from hessian.utils import tree_l2_norm_error\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e0717-71bf-4d15-be9e-47f506d47a19",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "This tutorial solves the following problem:\n",
    "\n",
    "Given a parameter vector $\\boldsymbol{\\theta}\\in\\mathbb{R}^2$, solve the residual equations $\\boldsymbol{F}(\\boldsymbol{u}, \\boldsymbol{\\theta})=\\boldsymbol{0}$ to get the solution vector $\\boldsymbol{u}\\in\\mathbb{R}^2$.\n",
    "\n",
    "Therefore, $\\boldsymbol{u}$ is a function of $\\boldsymbol{\\theta}$ implicitly, i.e., $\\boldsymbol{u}(\\boldsymbol{\\theta})$.\n",
    "\n",
    "Define some objective function $J(\\boldsymbol{u},\\boldsymbol{\\theta})$, the goal is to find the Hessian-vector product, i.e., $\\frac{\\textrm{d}^2 J}{\\textrm{d}\\boldsymbol{\\theta}^2} \\hat{\\boldsymbol{\\theta}}$ for any vector $\\hat{\\boldsymbol{\\theta}}\\in\\mathbb{R}^2$.\n",
    "\n",
    "We will define the specific forms of the residual function $\\boldsymbol{F}$ and the objective function $J$ in the subsequent codes.\n",
    "Three methods to find Hessian-vector product will be presented and compared:\n",
    "\n",
    "- **AD method**: Implicit differentiation approach with customized AD rules\n",
    "- **FD method**: Finite difference approximate approach\n",
    "- **AN method**: Analytical approach (available only for this particular simple problem)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf49ef-374a-4ebc-b40b-8d8590cca67f",
   "metadata": {},
   "source": [
    "## Residual equations and adjoint method\n",
    "\n",
    "The relationship between solution vector $\\boldsymbol{u} = [u_0, u_1]$ and parameter vector $\\boldsymbol{\\theta} = [\\theta_0, \\theta_1]$ is defined by the residual equations:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{F}(\\boldsymbol{u}, \\boldsymbol{\\theta}) = \\mathbf{0} \\quad \\Rightarrow \\quad\n",
    "\\begin{cases}\n",
    "F_0(u, \\theta) = \\theta_0^2 u_0 + \\theta_1 - 1 = 0 \\\\\n",
    "F_1(u, \\theta) = \\theta_1^2 u_0^2 + \\theta_1 u_1^2 - 2 = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Newton's method is used to solve $\\boldsymbol{F}(\\boldsymbol{u}, \\boldsymbol{\\theta}) = \\mathbf{0}$ for $\\boldsymbol{u}$ with given $\\boldsymbol{\\theta}$.\n",
    "- Adjoint method is used to get the adjoint vector $\\boldsymbol{\\lambda}$, which will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0df681-5dee-464a-92d6-5c149cc9a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(θ, u_init):\n",
    "    def F_fn(u, θ):\n",
    "        return np.array([θ[0]**2 * u[0] + θ[1] - 1, θ[1]**2 * u[0]**2 + θ[1] * u[1]**2 - 2])\n",
    "\n",
    "    _, unflatten = jax.flatten_util.ravel_pytree(u_init)\n",
    "\n",
    "    def u_fn(θ):\n",
    "        # Newton solve\n",
    "        tol = 1e-8\n",
    "        max_iter = 1000\n",
    "        u_flat, _ = jax.flatten_util.ravel_pytree(u_init)\n",
    "\n",
    "        def flat_F_fn(u_flat):\n",
    "            u = unflatten(u_flat)\n",
    "            F = F_fn(u, θ)\n",
    "            return jax.flatten_util.ravel_pytree(F)[0]\n",
    "\n",
    "        # Main Newton loop\n",
    "        for _ in range(max_iter):\n",
    "            # Compute current residual\n",
    "            F_flat = flat_F_fn(u_flat)\n",
    "            residual_norm = np.linalg.norm(F_flat)\n",
    "\n",
    "            print(f\"res = {residual_norm}\")\n",
    "            if residual_norm < tol:\n",
    "                break\n",
    "\n",
    "            # Compute dense Jacobian\n",
    "            J = jax.jacfwd(flat_F_fn)(u_flat)\n",
    "            \n",
    "            # Solve linear system (with small regularization for stability)\n",
    "            Δu_flat = np.linalg.solve(J, -F_flat)\n",
    "            \n",
    "            # Update solution\n",
    "            u_flat += Δu_flat\n",
    "\n",
    "        return unflatten(u_flat)\n",
    "\n",
    "    # Solve forward problem\n",
    "    print(f\"\\n################## Solve forward problem...\")\n",
    "    u = u_fn(θ)\n",
    "    print(f\"################## End of forward problem\\n\")\n",
    "   \n",
    "    return u, F_fn\n",
    "\n",
    "\n",
    "def adjoint_step(u, θ, J_fn, F_fn):\n",
    "    # Solve adjoint problem\n",
    "    _, unflatten = jax.flatten_util.ravel_pytree(u)\n",
    "\n",
    "    def flat_F_fn(u_flat):\n",
    "        u = unflatten(u_flat)\n",
    "        F = F_fn(u, θ)\n",
    "        return jax.flatten_util.ravel_pytree(F)[0]\n",
    "\n",
    "    print(f\"\\n################## Solve adjoint problem...\")\n",
    "    u_flat, _ = jax.flatten_util.ravel_pytree(u)\n",
    "\n",
    "    A = jax.jacfwd(flat_F_fn)(u_flat)\n",
    "\n",
    "    λ_rhs = jax.grad(J_fn)(u, θ)\n",
    "    λ_rhs_vec = jax.flatten_util.ravel_pytree(λ_rhs)[0]\n",
    "    λ_vec = np.linalg.solve(A.transpose(), -λ_rhs_vec)\n",
    "    λ = unflatten(λ_vec)\n",
    "    print(f\"################## End of adjoint problem\\n\")\n",
    "    \n",
    "    return λ, A\n",
    "\n",
    "\n",
    "def forward_and_adjoint(θ, J_fn, u_init):\n",
    "    u, F_fn = forward_step(θ, u_init)\n",
    "    λ, A = adjoint_step(u, θ, J_fn, F_fn)\n",
    "    return u, λ, F_fn, A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2ab3f-3d98-4d49-8d08-ec8cd0d296a2",
   "metadata": {},
   "source": [
    "## AD method\n",
    "\n",
    "We build a `HessVecProduct` class as a manager, which can be handy for optimization (e.g., interface with the `scipy` package). Optimization itself is not covered in this tutorial, since we only focus on Hessian-vector products here.\n",
    "\n",
    "Note that the function `incremental_forward_and_adjoint` is called from the `hessian.hessp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbe8d60-04b0-4554-8394-4ababb96302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HessVecProduct:\n",
    "    def __init__(self, u_init, J_fn):\n",
    "        self.internal_vars = {'θ': None, 'u': None, 'λ': None, 'F_fn': None, 'A': None}\n",
    "        self.J_fn = J_fn\n",
    "        self.state_linear_solver = lambda A, b: np.linalg.solve(A, b)\n",
    "        self.adjoint_linear_solver = lambda A, b: np.linalg.solve(A, b)\n",
    "        \n",
    "    def J(self, θ):\n",
    "        u, F_fn = forward_step(θ, u_init)\n",
    "        return self.J_fn(u, θ)\n",
    "\n",
    "    def grad(self, θ):\n",
    "        u, λ, F_fn, A = forward_and_adjoint(θ, self.J_fn, u_init)\n",
    "        primals_out, f_vjp = jax.vjp(lambda θ: F_fn(u, θ), θ) # λ_i * (∂/∂θ_k)F_i\n",
    "        vjp_θ, = f_vjp(λ)\n",
    "        dJ_dθ = jax.grad(self.J_fn, argnums=1)(u, θ) # ∂J/∂θ_k\n",
    "        vjp_result = jax.tree_util.tree_map(lambda x, y: x + y, vjp_θ, dJ_dθ) # dJ/dθ_k\n",
    "        return vjp_result\n",
    "\n",
    "    def hessp(self, θ, θ_hat):\n",
    "        tol = 1e-8\n",
    "        if (self.internal_vars['θ'] is None) or tree_l2_norm_error(self.internal_vars['θ'], θ) > tol:\n",
    "            print(f\"hessp needs to solve forward and adjoint problem...\")\n",
    "            u, λ, F_fn, A = forward_and_adjoint(θ, self.J_fn, u_init)\n",
    "            self.internal_vars['θ'] = θ\n",
    "            self.internal_vars['u'] = u\n",
    "            self.internal_vars['λ'] = λ\n",
    "            self.internal_vars['F_fn'] = F_fn\n",
    "            self.internal_vars['A'] = A\n",
    "        else:\n",
    "            print(f\"hessp does NOT need to solve forward and adjoint problem...\")\n",
    "            θ = self.internal_vars['θ']\n",
    "            u = self.internal_vars['u']\n",
    "            λ = self.internal_vars['λ']\n",
    "            F_fn = self.internal_vars['F_fn']\n",
    "            A = self.internal_vars['A']\n",
    "\n",
    "        print(f\"\\n################## Solve incremental forward and adjoint problem...\")\n",
    "        dθ_dθ_J_θ_hat, profile_info = incremental_forward_and_adjoint(u, θ, λ, θ_hat, self.J_fn, F_fn, A, \n",
    "                                                                      self.state_linear_solver, self.adjoint_linear_solver, \n",
    "                                                                      option='rev_rev') # change 'option' for other modes\n",
    "        print(f\"################## End of incremental forward and adjoint problem\\n\")\n",
    "        return dθ_dθ_J_θ_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaea3f0-5e74-4e90-ab39-1af43c2ee3d9",
   "metadata": {},
   "source": [
    "## FD method\n",
    "\n",
    "Finite difference gives approximate Hessian-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fee2e58-2586-48ba-9f3b-4e5420502060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_hessp(hess_vec_prod, θ, θ_hat):\n",
    "    h = 1e-6\n",
    "    θ_minus = jax.tree_util.tree_map(lambda x, y: x - h*y, θ, θ_hat)\n",
    "    θ_plus  = jax.tree_util.tree_map(lambda x, y: x + h*y, θ, θ_hat)\n",
    "    value_plus = hess_vec_prod.grad(θ_plus)\n",
    "    value_minus = hess_vec_prod.grad(θ_minus)\n",
    "    dθ_dθ_J_θ_hat = jax.tree_util.tree_map(lambda x, y: (x - y)/(2*h), value_plus, value_minus)\n",
    "    return dθ_dθ_J_θ_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8e541-9dbf-44e1-8397-d9056adc9378",
   "metadata": {},
   "source": [
    "## AN method\n",
    "\n",
    "For this particular problem, it is easy to analytically obtain\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}(\\boldsymbol{\\theta)} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{1 - \\theta_1}{\\theta_0^2} \\\\\n",
    "\\sqrt{\\frac{2 - \\left(\\frac{1 - \\theta_1}{\\theta_0^2}\\right)^2 \\theta_1^2}{\\theta_1}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we can let JAX compute the Hessian-vector products for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f06a9d-cc9c-440c-b357-d088bb66bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_hessp(θ, θ_hat, J_fn):\n",
    "    def u_fn(θ):\n",
    "        return np.array([(1. - θ[1])/θ[0]**2, np.sqrt( (2. -  ((1. - θ[1])/θ[0]**2)**2 * θ[1]**2) / θ[1] )])\n",
    "\n",
    "    def J(θ):\n",
    "        return J_fn(u_fn(θ), θ)\n",
    "\n",
    "    dθ_dθ_J_θ_hat = jax.jacrev(jax.grad(J))(θ) @ θ_hat\n",
    "    return dθ_dθ_J_θ_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1af5ae-2cd1-4bfd-b536-dcf7b383c7a2",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "\n",
    "Let us define the objective function as\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{u},\\boldsymbol{\\theta}) = u_0^3 + u_1^3 + \\theta_0^3 + \\theta_1^3 + (u_0^2 + u_1^2)(e^{\\theta_0} + e^{\\theta_1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae95c17-030b-464e-b837-ccc4c2627827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_fn(u, θ):\n",
    "    u_vec = jax.flatten_util.ravel_pytree(u)[0]\n",
    "    θ_vec = jax.flatten_util.ravel_pytree(θ)[0]\n",
    "    return np.sum(u_vec**3) + np.sum(θ_vec**3) + np.sum(u_vec**2) * np.sum(np.exp(θ_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e19f4-270f-4676-a41c-137e4bfc7a0e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We show the three approaches and the corresponding Hessian-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26dddfb1-42b3-4ee6-ac25-ced473fcd97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-18 14:07:43][DEBUG] jax_fem: ################## Solve incremental forward problem...\n",
      "[05-18 14:07:43][DEBUG] jax_fem: ################## Solve incremental adjoint problem...\n",
      "[05-18 14:07:43][DEBUG] jax_fem: rev_rev: time elapsed for J-related evaluation is 0.0391595340000066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************************************************************************\n",
      "Running AD-based approach to find Hessian-vector product...\n",
      "hessp needs to solve forward and adjoint problem...\n",
      "\n",
      "################## Solve forward problem...\n",
      "res = 2.0001014374276123\n",
      "res = 498.8451170370369\n",
      "res = 124.21335426711808\n",
      "res = 30.561338186930122\n",
      "res = 7.171115420075269\n",
      "res = 1.401865156171163\n",
      "res = 0.14443609767126464\n",
      "res = 0.0024324414958898366\n",
      "res = 7.388146427977915e-07\n",
      "res = 6.88338275267597e-14\n",
      "################## End of forward problem\n",
      "\n",
      "\n",
      "################## Solve adjoint problem...\n",
      "################## End of adjoint problem\n",
      "\n",
      "\n",
      "################## Solve incremental forward and adjoint problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[05-18 14:07:43][DEBUG] jax_fem: rev_rev: time elapsed for F-related evaluation is 0.06895638000000304\n",
      "[05-18 14:07:43][DEBUG] jax_fem: ################## Find hessian-vector product...\n",
      "[05-18 14:07:43][DEBUG] jax_fem: ################## Finshed using AD to find HVP.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## End of incremental forward and adjoint problem\n",
      "\n",
      "\n",
      "End of AD-based approach to find Hessian-vector product\n",
      "************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************\n",
      "Running FD-based approach to find Hessian-vector product...\n",
      "\n",
      "################## Solve forward problem...\n",
      "res = 2.000101454231843\n",
      "res = 498.84436698668696\n",
      "res = 124.21316675767527\n",
      "res = 30.56129132148854\n",
      "res = 7.1711037480425155\n",
      "res = 1.4018623769967844\n",
      "res = 0.1444356430226601\n",
      "res = 0.0024324266989759202\n",
      "res = 7.388056602053439e-07\n",
      "res = 6.794564910705958e-14\n",
      "################## End of forward problem\n",
      "\n",
      "\n",
      "################## Solve adjoint problem...\n",
      "################## End of adjoint problem\n",
      "\n",
      "\n",
      "################## Solve forward problem...\n",
      "res = 2.0001014206234675\n",
      "res = 498.8458670896375\n",
      "res = 124.2135417771237\n",
      "res = 30.561385052512392\n",
      "res = 7.171127092143186\n",
      "res = 1.4018679353542702\n",
      "res = 0.1444365523216904\n",
      "res = 0.0024324562929045612\n",
      "res = 7.388236262784176e-07\n",
      "res = 6.838973831690964e-14\n",
      "################## End of forward problem\n",
      "\n",
      "\n",
      "################## Solve adjoint problem...\n",
      "################## End of adjoint problem\n",
      "\n",
      "\n",
      "End of FD-based approach to find Hessian-vector product\n",
      "************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "************************************************************************\n",
      "Running AN-based approach to find Hessian-vector product...\n",
      "\n",
      "End of AN-based approach to find Hessian-vector product\n",
      "************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "Results:\n",
      "AD = [-257.43943628 3852.39760026] (automatic differentiation)\n",
      "FD = [-257.43943633 3852.39760055] (finite difference)\n",
      "AN = [-257.43943628 3852.39760026] (analytical)\n"
     ]
    }
   ],
   "source": [
    "θ = np.array([3., .2])\n",
    "θ_hat = np.array([0.2, 0.3])\n",
    "u_init = np.array([0.1, 0.1])\n",
    "\n",
    "hess_vec_prod = HessVecProduct(u_init, J_fn)\n",
    "\n",
    "# Automatic differentiation\n",
    "print(f\"\\n\\n************************************************************************\")\n",
    "print(f\"Running AD-based approach to find Hessian-vector product...\")\n",
    "dθ_dθ_J_θ_hat_ad = hess_vec_prod.hessp(θ, θ_hat)\n",
    "print(f\"\\nEnd of AD-based approach to find Hessian-vector product\")\n",
    "print(f\"************************************************************************\\n\\n\")\n",
    "\n",
    "# Finite difference\n",
    "print(f\"\\n\\n************************************************************************\")\n",
    "print(f\"Running FD-based approach to find Hessian-vector product...\")\n",
    "dθ_dθ_J_θ_hat_fd = finite_difference_hessp(hess_vec_prod, θ, θ_hat)\n",
    "print(f\"\\nEnd of FD-based approach to find Hessian-vector product\")\n",
    "print(f\"************************************************************************\\n\\n\")\n",
    "\n",
    "# Analytical\n",
    "print(f\"\\n\\n************************************************************************\")\n",
    "print(f\"Running AN-based approach to find Hessian-vector product...\")\n",
    "dθ_dθ_J_θ_hat_an = analytical_hessp(θ, θ_hat, J_fn)\n",
    "print(f\"\\nEnd of AN-based approach to find Hessian-vector product\")\n",
    "print(f\"************************************************************************\\n\\n\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"AD = {dθ_dθ_J_θ_hat_ad} (automatic differentiation)\")\n",
    "print(f\"FD = {dθ_dθ_J_θ_hat_fd} (finite difference)\")\n",
    "print(f\"AN = {dθ_dθ_J_θ_hat_an} (analytical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752280a-b87f-4401-b722-4663dade0842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
